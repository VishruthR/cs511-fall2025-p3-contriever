{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125610d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting beir\n",
      "  Downloading beir-2.2.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 KB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /users/vishraj/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 KB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 KB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.22.4\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvshmem-cu12==3.3.20\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting networkx>=2.5.1\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 KB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.8.93\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /users/vishraj/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Collecting fsspec>=0.8.5\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 KB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.5.1\n",
      "  Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.27.5\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 KB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 KB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /users/vishraj/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 KB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.0/488.0 KB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytrec-eval-terrier\n",
      "  Downloading pytrec_eval_terrier-0.5.10-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (303 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.7/303.7 KB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 KB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (59.6.0)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.24.0,>=3.19.6\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 KB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cramjam>=2.3\n",
      "  Downloading cramjam-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.4.1,>=0.3.0\n",
      "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx<1.0.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.19\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting anyio\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna\n",
      "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/153.6 KB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
      "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.0/347.0 KB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.5/219.5 KB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.9/196.9 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /users/vishraj/.local/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, xxhash, urllib3, tzdata, triton, tqdm, threadpoolctl, tensorboard-data-server, sympy, sniffio, safetensors, regex, pyarrow, protobuf, propcache, pillow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, markdown, joblib, idna, hf-xet, h11, grpcio, fsspec, frozenlist, filelock, dill, cramjam, charset_normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, absl-py, yarl, werkzeug, scipy, requests, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, httpcore, faiss-cpu, anyio, aiosignal, tensorboard, scikit-learn, pytrec-eval-terrier, nvidia-cusolver-cu12, huggingface-hub, httpx, fastparquet, aiohttp, torch, tokenizers, transformers, datasets, sentence-transformers, beir\n",
      "Successfully installed MarkupSafe-3.0.3 absl-py-2.3.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 async-timeout-5.0.1 attrs-25.4.0 beir-2.2.0 certifi-2025.11.12 charset_normalizer-3.4.4 cramjam-2.11.0 datasets-4.4.1 dill-0.4.0 faiss-cpu-1.13.0 fastparquet-2024.11.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 grpcio-1.76.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 markdown-3.10 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.18 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 pillow-12.0.0 propcache-0.4.1 protobuf-6.33.1 pyarrow-22.0.0 pytrec-eval-terrier-0.5.10 pytz-2025.2 regex-2025.11.3 requests-2.32.5 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.15.3 sentence-transformers-5.1.2 sniffio-1.3.1 sympy-1.14.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.1 triton-3.5.1 tzdata-2025.2 urllib3-2.5.0 werkzeug-3.1.3 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A30\n",
      "GPU Memory: 25.3 GB\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas torch transformers beir scikit-learn scipy faiss-cpu tensorboard pyarrow fastparquet datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0380a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A30\n",
      "GPU Memory: 25.3 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c58a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./wikipedia_en_20231101_subset.txt\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/000...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/000.pkl...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/008...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/004...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/002...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/003...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/007...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/001...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/005...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/006...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/002.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/006.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/008.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/003.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/007.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/001.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/005.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/004.pkl...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/016...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/012...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/014...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/011...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/009...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/015...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/013...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/010...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/012.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/015.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/013.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/009.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/016.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/010.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/014.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/011.pkl...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/022...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/019...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/018...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/020...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/021...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/017...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/023...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/024...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/021.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/019.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/022.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/017.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/018.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/023.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/020.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/024.pkl...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/026...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/032...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/028...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/027...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/031...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/029...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/025...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/030...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/026.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/028.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/031.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/030.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/032.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/029.pkl...\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/027.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Saving at ./encoded-data/bert-base-uncased/wikipedia_en_20231101_subset/025.pkl...\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 76, in <module>\n",
      "    tokenize_file(args)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 64, in tokenize_file\n",
      "    save(tokens, savepath)\n",
      "  File \"/users/vishraj/cs511-fall2025-p3-contriever/./scripts/preprocess/preprocess.py\", line 18, in save\n",
      "    torch.save(tensor, fout)\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "  File \"/users/vishraj/.local/lib/python3.10/site-packages/torch/serialization.py\", line 818, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 704 vs 598\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/039...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/033...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/036...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/040...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/037...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/038...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/035...\n",
      "Encoding ./tmp-tokenization-bert-base-uncased-wikipedia_en_20231101_subset.txt/034...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "#Tokenize data\n",
    "#Place datasets under contriever/scripts/preprocess/\n",
    "!bash ./scripts/preprocess/tokenization_script.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded95ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "                            augmentation: delete                                  \t[default: none]\n",
      "                                   beta1: 0.9                                     \n",
      "                                   beta2: 0.98                                    \n",
      "                            chunk_length: 256                                     \n",
      "                       continue_training: False                                   \n",
      "                        contrastive_mode: moco                                    \n",
      "                                 dropout: 0.1                                     \n",
      "                                     eps: 1e-06                                   \n",
      "                               eval_data: []                                      \n",
      "                           eval_datasets: []                                      \n",
      "                       eval_datasets_dir: ./                                      \n",
      "                               eval_freq: 500                                     \n",
      "                     eval_normalize_text: False                                   \n",
      "                         label_smoothing: 0.0                                     \n",
      "                            loading_mode: split                                   \n",
      "                              local_rank: -1                                      \n",
      "                                log_freq: 100                                     \n",
      "                              lower_case: False                                   \n",
      "                                      lr: 5e-05                                   \t[default: 0.0001]\n",
      "                            lr_min_ratio: 0.0                                     \n",
      "                               main_port: 10001                                   \n",
      "                                 maxload: None                                    \n",
      "               moco_train_mode_encoder_k: False                                   \n",
      "                              model_path: none                                    \n",
      "                                momentum: 0.9995                                  \t[default: 0.999]\n",
      "                           negative_ctxs: 1                                       \n",
      "                   negative_hard_min_idx: 0                                       \n",
      "                     negative_hard_ratio: 0.0                                     \n",
      "                                norm_doc: False                                   \n",
      "                              norm_query: False                                   \n",
      "                             num_workers: 5                                       \n",
      "                                   optim: adamw                                   \n",
      "                              output_dir: /users/vishraj/cs511-fall2025-p3-contriever/model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1\t[default: ./checkpoint/my_experiments]\n",
      "                      per_gpu_batch_size: 128                                     \t[default: 64]\n",
      "                 per_gpu_eval_batch_size: 256                                     \n",
      "                                 pooling: average                                 \n",
      "                       prob_augmentation: 0.1                                     \t[default: 0.0]\n",
      "                         projection_size: 768                                     \n",
      "                              queue_size: 4096                                    \t[default: 65536]\n",
      "                             random_init: False                                   \n",
      "                               ratio_max: 0.5                                     \n",
      "                               ratio_min: 0.05                                    \t[default: 0.1]\n",
      "                      retriever_model_id: bert-base-uncased                       \n",
      "                                     rho: 0.05                                    \n",
      "                    sampling_coefficient: 0.0                                     \n",
      "                               save_freq: 50000                                   \n",
      "                               scheduler: linear                                  \n",
      "                          score_function: dot                                     \n",
      "                                    seed: 0                                       \n",
      "                             temperature: 0.05                                    \t[default: 1.0]\n",
      "                             total_steps: 100000                                  \t[default: 1000]\n",
      "                              train_data: ['/users/vishraj/cs511-fall2025-p3-contriever/encoded-data-small/bert-base-uncased/wikipedia_en_20231101_subset']\t[default: []]\n",
      "                            warmup_steps: 100                                     \t[default: -1]\n",
      "                            weight_decay: 0.01                                    \n",
      "\n",
      "directory_exists: False\n",
      "checkpoint_exists: False\n",
      "the directory is :/users/vishraj/cs511-fall2025-p3-contriever/model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1\n",
      "[11/18/2025 14:23:57] {train.py:208} INFO - [Network] Total number of parameters : 217.783296 M\n",
      "[11/18/2025 14:23:57] {train.py:210} INFO - Start training\n",
      "[11/18/2025 14:23:57] {train.py:45} INFO - Data loading\n",
      "[11/18/2025 14:23:58] {train.py:52} WARNING - Data loading finished for rank 0\n",
      "[11/18/2025 14:23:58] {train.py:53} INFO - Train dataset length: 256655\n",
      "[11/18/2025 14:23:58] {train.py:71} INFO - Start epoch 1\n",
      "[11/18/2025 14:23:58] {train.py:73} INFO - Batch 0/2005 (epoch 1)\n",
      "[11/18/2025 14:24:00] {train.py:73} INFO - Batch 1/2005 (epoch 1)\n",
      "[11/18/2025 14:24:01] {train.py:73} INFO - Batch 2/2005 (epoch 1)\n",
      "[11/18/2025 14:24:02] {train.py:73} INFO - Batch 3/2005 (epoch 1)\n",
      "[11/18/2025 14:24:04] {train.py:73} INFO - Batch 4/2005 (epoch 1)\n",
      "[11/18/2025 14:24:05] {train.py:73} INFO - Batch 5/2005 (epoch 1)\n",
      "[11/18/2025 14:24:06] {train.py:73} INFO - Batch 6/2005 (epoch 1)\n",
      "[11/18/2025 14:24:07] {train.py:73} INFO - Batch 7/2005 (epoch 1)\n",
      "[11/18/2025 14:24:09] {train.py:73} INFO - Batch 8/2005 (epoch 1)\n",
      "[11/18/2025 14:24:10] {train.py:73} INFO - Batch 9/2005 (epoch 1)\n",
      "[11/18/2025 14:24:11] {train.py:73} INFO - Batch 10/2005 (epoch 1)\n",
      "[11/18/2025 14:24:12] {train.py:73} INFO - Batch 11/2005 (epoch 1)\n",
      "[11/18/2025 14:24:14] {train.py:73} INFO - Batch 12/2005 (epoch 1)\n",
      "[11/18/2025 14:24:15] {train.py:73} INFO - Batch 13/2005 (epoch 1)\n",
      "[11/18/2025 14:24:16] {train.py:73} INFO - Batch 14/2005 (epoch 1)\n",
      "[11/18/2025 14:24:17] {train.py:73} INFO - Batch 15/2005 (epoch 1)\n",
      "[11/18/2025 14:24:19] {train.py:73} INFO - Batch 16/2005 (epoch 1)\n",
      "[11/18/2025 14:24:20] {train.py:73} INFO - Batch 17/2005 (epoch 1)\n",
      "[11/18/2025 14:24:21] {train.py:73} INFO - Batch 18/2005 (epoch 1)\n",
      "[11/18/2025 14:24:22] {train.py:73} INFO - Batch 19/2005 (epoch 1)\n",
      "[11/18/2025 14:24:24] {train.py:73} INFO - Batch 20/2005 (epoch 1)\n",
      "[11/18/2025 14:24:25] {train.py:73} INFO - Batch 21/2005 (epoch 1)\n",
      "[11/18/2025 14:24:26] {train.py:73} INFO - Batch 22/2005 (epoch 1)\n",
      "[11/18/2025 14:24:27] {train.py:73} INFO - Batch 23/2005 (epoch 1)\n",
      "[11/18/2025 14:24:29] {train.py:73} INFO - Batch 24/2005 (epoch 1)\n",
      "[11/18/2025 14:24:30] {train.py:73} INFO - Batch 25/2005 (epoch 1)\n",
      "[11/18/2025 14:24:31] {train.py:73} INFO - Batch 26/2005 (epoch 1)\n",
      "[11/18/2025 14:24:33] {train.py:73} INFO - Batch 27/2005 (epoch 1)\n",
      "[11/18/2025 14:24:34] {train.py:73} INFO - Batch 28/2005 (epoch 1)\n",
      "[11/18/2025 14:24:35] {train.py:73} INFO - Batch 29/2005 (epoch 1)\n",
      "[11/18/2025 14:24:36] {train.py:73} INFO - Batch 30/2005 (epoch 1)\n",
      "[11/18/2025 14:24:38] {train.py:73} INFO - Batch 31/2005 (epoch 1)\n",
      "[11/18/2025 14:24:39] {train.py:73} INFO - Batch 32/2005 (epoch 1)\n",
      "[11/18/2025 14:24:40] {train.py:73} INFO - Batch 33/2005 (epoch 1)\n",
      "[11/18/2025 14:24:41] {train.py:73} INFO - Batch 34/2005 (epoch 1)\n",
      "[11/18/2025 14:24:43] {train.py:73} INFO - Batch 35/2005 (epoch 1)\n",
      "[11/18/2025 14:24:44] {train.py:73} INFO - Batch 36/2005 (epoch 1)\n",
      "[11/18/2025 14:24:45] {train.py:73} INFO - Batch 37/2005 (epoch 1)\n",
      "[11/18/2025 14:24:47] {train.py:73} INFO - Batch 38/2005 (epoch 1)\n",
      "[11/18/2025 14:24:48] {train.py:73} INFO - Batch 39/2005 (epoch 1)\n",
      "[11/18/2025 14:24:49] {train.py:73} INFO - Batch 40/2005 (epoch 1)\n",
      "[11/18/2025 14:24:50] {train.py:73} INFO - Batch 41/2005 (epoch 1)\n",
      "[11/18/2025 14:24:52] {train.py:73} INFO - Batch 42/2005 (epoch 1)\n",
      "[11/18/2025 14:24:53] {train.py:73} INFO - Batch 43/2005 (epoch 1)\n",
      "[11/18/2025 14:24:54] {train.py:73} INFO - Batch 44/2005 (epoch 1)\n",
      "[11/18/2025 14:24:55] {train.py:73} INFO - Batch 45/2005 (epoch 1)\n",
      "[11/18/2025 14:24:57] {train.py:73} INFO - Batch 46/2005 (epoch 1)\n",
      "[11/18/2025 14:24:58] {train.py:73} INFO - Batch 47/2005 (epoch 1)\n",
      "[11/18/2025 14:24:59] {train.py:73} INFO - Batch 48/2005 (epoch 1)\n",
      "[11/18/2025 14:25:01] {train.py:73} INFO - Batch 49/2005 (epoch 1)\n",
      "[11/18/2025 14:25:02] {train.py:73} INFO - Batch 50/2005 (epoch 1)\n",
      "[11/18/2025 14:25:03] {train.py:73} INFO - Batch 51/2005 (epoch 1)\n",
      "[11/18/2025 14:25:04] {train.py:73} INFO - Batch 52/2005 (epoch 1)\n",
      "[11/18/2025 14:25:06] {train.py:73} INFO - Batch 53/2005 (epoch 1)\n",
      "[11/18/2025 14:25:07] {train.py:73} INFO - Batch 54/2005 (epoch 1)\n",
      "[11/18/2025 14:25:08] {train.py:73} INFO - Batch 55/2005 (epoch 1)\n",
      "[11/18/2025 14:25:09] {train.py:73} INFO - Batch 56/2005 (epoch 1)\n",
      "[11/18/2025 14:25:11] {train.py:73} INFO - Batch 57/2005 (epoch 1)\n",
      "[11/18/2025 14:25:12] {train.py:73} INFO - Batch 58/2005 (epoch 1)\n",
      "[11/18/2025 14:25:13] {train.py:73} INFO - Batch 59/2005 (epoch 1)\n",
      "[11/18/2025 14:25:15] {train.py:73} INFO - Batch 60/2005 (epoch 1)\n",
      "[11/18/2025 14:25:16] {train.py:73} INFO - Batch 61/2005 (epoch 1)\n",
      "[11/18/2025 14:25:17] {train.py:73} INFO - Batch 62/2005 (epoch 1)\n",
      "[11/18/2025 14:25:18] {train.py:73} INFO - Batch 63/2005 (epoch 1)\n",
      "[11/18/2025 14:25:20] {train.py:73} INFO - Batch 64/2005 (epoch 1)\n",
      "[11/18/2025 14:25:21] {train.py:73} INFO - Batch 65/2005 (epoch 1)\n",
      "[11/18/2025 14:25:22] {train.py:73} INFO - Batch 66/2005 (epoch 1)\n",
      "[11/18/2025 14:25:23] {train.py:73} INFO - Batch 67/2005 (epoch 1)\n",
      "[11/18/2025 14:25:25] {train.py:73} INFO - Batch 68/2005 (epoch 1)\n",
      "[11/18/2025 14:25:26] {train.py:73} INFO - Batch 69/2005 (epoch 1)\n",
      "[11/18/2025 14:25:27] {train.py:73} INFO - Batch 70/2005 (epoch 1)\n",
      "[11/18/2025 14:25:29] {train.py:73} INFO - Batch 71/2005 (epoch 1)\n",
      "[11/18/2025 14:25:30] {train.py:73} INFO - Batch 72/2005 (epoch 1)\n",
      "[11/18/2025 14:25:31] {train.py:73} INFO - Batch 73/2005 (epoch 1)\n",
      "[11/18/2025 14:25:32] {train.py:73} INFO - Batch 74/2005 (epoch 1)\n",
      "[11/18/2025 14:25:34] {train.py:73} INFO - Batch 75/2005 (epoch 1)\n",
      "[11/18/2025 14:25:35] {train.py:73} INFO - Batch 76/2005 (epoch 1)\n",
      "[11/18/2025 14:25:36] {train.py:73} INFO - Batch 77/2005 (epoch 1)\n",
      "[11/18/2025 14:25:38] {train.py:73} INFO - Batch 78/2005 (epoch 1)\n",
      "[11/18/2025 14:25:39] {train.py:73} INFO - Batch 79/2005 (epoch 1)\n",
      "[11/18/2025 14:25:40] {train.py:73} INFO - Batch 80/2005 (epoch 1)\n",
      "[11/18/2025 14:25:41] {train.py:73} INFO - Batch 81/2005 (epoch 1)\n",
      "[11/18/2025 14:25:43] {train.py:73} INFO - Batch 82/2005 (epoch 1)\n",
      "[11/18/2025 14:25:44] {train.py:73} INFO - Batch 83/2005 (epoch 1)\n",
      "[11/18/2025 14:25:45] {train.py:73} INFO - Batch 84/2005 (epoch 1)\n",
      "[11/18/2025 14:25:47] {train.py:73} INFO - Batch 85/2005 (epoch 1)\n",
      "[11/18/2025 14:25:48] {train.py:73} INFO - Batch 86/2005 (epoch 1)\n",
      "[11/18/2025 14:25:49] {train.py:73} INFO - Batch 87/2005 (epoch 1)\n",
      "[11/18/2025 14:25:50] {train.py:73} INFO - Batch 88/2005 (epoch 1)\n",
      "[11/18/2025 14:25:52] {train.py:73} INFO - Batch 89/2005 (epoch 1)\n",
      "[11/18/2025 14:25:53] {train.py:73} INFO - Batch 90/2005 (epoch 1)\n",
      "[11/18/2025 14:25:54] {train.py:73} INFO - Batch 91/2005 (epoch 1)\n",
      "[11/18/2025 14:25:56] {train.py:73} INFO - Batch 92/2005 (epoch 1)\n",
      "[11/18/2025 14:25:57] {train.py:73} INFO - Batch 93/2005 (epoch 1)\n",
      "[11/18/2025 14:25:58] {train.py:73} INFO - Batch 94/2005 (epoch 1)\n",
      "[11/18/2025 14:25:59] {train.py:73} INFO - Batch 95/2005 (epoch 1)\n",
      "[11/18/2025 14:26:01] {train.py:73} INFO - Batch 96/2005 (epoch 1)\n",
      "[11/18/2025 14:26:02] {train.py:73} INFO - Batch 97/2005 (epoch 1)\n",
      "[11/18/2025 14:26:03] {train.py:73} INFO - Batch 98/2005 (epoch 1)\n",
      "[11/18/2025 14:26:04] {train.py:73} INFO - Batch 99/2005 (epoch 1)\n",
      "[11/18/2025 14:26:06] {train.py:100} INFO - 100 / 100000 | train/accuracy: 51.555 | train/loss: 30.223 | train/stdk: 0.190 | train/stdq: 0.119 | lr: 5e-05 | Memory: 12.0 GiB\n",
      "[11/18/2025 14:26:06] {train.py:73} INFO - Batch 100/2005 (epoch 1)\n",
      "[11/18/2025 14:26:07] {train.py:73} INFO - Batch 101/2005 (epoch 1)\n",
      "[11/18/2025 14:26:08] {train.py:73} INFO - Batch 102/2005 (epoch 1)\n",
      "[11/18/2025 14:26:10] {train.py:73} INFO - Batch 103/2005 (epoch 1)\n",
      "[11/18/2025 14:26:11] {train.py:73} INFO - Batch 104/2005 (epoch 1)\n",
      "[11/18/2025 14:26:12] {train.py:73} INFO - Batch 105/2005 (epoch 1)\n",
      "[11/18/2025 14:26:13] {train.py:73} INFO - Batch 106/2005 (epoch 1)\n",
      "[11/18/2025 14:26:15] {train.py:73} INFO - Batch 107/2005 (epoch 1)\n",
      "[11/18/2025 14:26:16] {train.py:73} INFO - Batch 108/2005 (epoch 1)\n",
      "[11/18/2025 14:26:17] {train.py:73} INFO - Batch 109/2005 (epoch 1)\n",
      "[11/18/2025 14:26:18] {train.py:73} INFO - Batch 110/2005 (epoch 1)\n",
      "[11/18/2025 14:26:20] {train.py:73} INFO - Batch 111/2005 (epoch 1)\n",
      "[11/18/2025 14:26:21] {train.py:73} INFO - Batch 112/2005 (epoch 1)\n",
      "[11/18/2025 14:26:22] {train.py:73} INFO - Batch 113/2005 (epoch 1)\n",
      "[11/18/2025 14:26:24] {train.py:73} INFO - Batch 114/2005 (epoch 1)\n",
      "[11/18/2025 14:26:25] {train.py:73} INFO - Batch 115/2005 (epoch 1)\n",
      "[11/18/2025 14:26:26] {train.py:73} INFO - Batch 116/2005 (epoch 1)\n",
      "[11/18/2025 14:26:28] {train.py:73} INFO - Batch 117/2005 (epoch 1)\n",
      "[11/18/2025 14:26:29] {train.py:73} INFO - Batch 118/2005 (epoch 1)\n",
      "[11/18/2025 14:26:30] {train.py:73} INFO - Batch 119/2005 (epoch 1)\n",
      "[11/18/2025 14:26:31] {train.py:73} INFO - Batch 120/2005 (epoch 1)\n",
      "[11/18/2025 14:26:33] {train.py:73} INFO - Batch 121/2005 (epoch 1)\n",
      "[11/18/2025 14:26:34] {train.py:73} INFO - Batch 122/2005 (epoch 1)\n",
      "[11/18/2025 14:26:35] {train.py:73} INFO - Batch 123/2005 (epoch 1)\n",
      "[11/18/2025 14:26:37] {train.py:73} INFO - Batch 124/2005 (epoch 1)\n",
      "[11/18/2025 14:26:38] {train.py:73} INFO - Batch 125/2005 (epoch 1)\n",
      "[11/18/2025 14:26:39] {train.py:73} INFO - Batch 126/2005 (epoch 1)\n",
      "[11/18/2025 14:26:40] {train.py:73} INFO - Batch 127/2005 (epoch 1)\n",
      "[11/18/2025 14:26:42] {train.py:73} INFO - Batch 128/2005 (epoch 1)\n",
      "[11/18/2025 14:26:43] {train.py:73} INFO - Batch 129/2005 (epoch 1)\n",
      "[11/18/2025 14:26:44] {train.py:73} INFO - Batch 130/2005 (epoch 1)\n",
      "[11/18/2025 14:26:46] {train.py:73} INFO - Batch 131/2005 (epoch 1)\n",
      "[11/18/2025 14:26:47] {train.py:73} INFO - Batch 132/2005 (epoch 1)\n",
      "[11/18/2025 14:26:48] {train.py:73} INFO - Batch 133/2005 (epoch 1)\n",
      "[11/18/2025 14:26:50] {train.py:73} INFO - Batch 134/2005 (epoch 1)\n",
      "[11/18/2025 14:26:51] {train.py:73} INFO - Batch 135/2005 (epoch 1)\n",
      "[11/18/2025 14:26:52] {train.py:73} INFO - Batch 136/2005 (epoch 1)\n",
      "[11/18/2025 14:26:53] {train.py:73} INFO - Batch 137/2005 (epoch 1)\n",
      "[11/18/2025 14:26:55] {train.py:73} INFO - Batch 138/2005 (epoch 1)\n",
      "[11/18/2025 14:26:56] {train.py:73} INFO - Batch 139/2005 (epoch 1)\n",
      "[11/18/2025 14:26:57] {train.py:73} INFO - Batch 140/2005 (epoch 1)\n",
      "[11/18/2025 14:26:59] {train.py:73} INFO - Batch 141/2005 (epoch 1)\n",
      "[11/18/2025 14:27:00] {train.py:73} INFO - Batch 142/2005 (epoch 1)\n",
      "[11/18/2025 14:27:01] {train.py:73} INFO - Batch 143/2005 (epoch 1)\n",
      "[11/18/2025 14:27:02] {train.py:73} INFO - Batch 144/2005 (epoch 1)\n",
      "[11/18/2025 14:27:04] {train.py:73} INFO - Batch 145/2005 (epoch 1)\n",
      "[11/18/2025 14:27:05] {train.py:73} INFO - Batch 146/2005 (epoch 1)\n",
      "[11/18/2025 14:27:06] {train.py:73} INFO - Batch 147/2005 (epoch 1)\n",
      "[11/18/2025 14:27:08] {train.py:73} INFO - Batch 148/2005 (epoch 1)\n",
      "[11/18/2025 14:27:09] {train.py:73} INFO - Batch 149/2005 (epoch 1)\n",
      "[11/18/2025 14:27:10] {train.py:73} INFO - Batch 150/2005 (epoch 1)\n",
      "[11/18/2025 14:27:11] {train.py:73} INFO - Batch 151/2005 (epoch 1)\n",
      "[11/18/2025 14:27:13] {train.py:73} INFO - Batch 152/2005 (epoch 1)\n",
      "[11/18/2025 14:27:14] {train.py:73} INFO - Batch 153/2005 (epoch 1)\n",
      "[11/18/2025 14:27:15] {train.py:73} INFO - Batch 154/2005 (epoch 1)\n",
      "[11/18/2025 14:27:17] {train.py:73} INFO - Batch 155/2005 (epoch 1)\n",
      "[11/18/2025 14:27:18] {train.py:73} INFO - Batch 156/2005 (epoch 1)\n",
      "[11/18/2025 14:27:19] {train.py:73} INFO - Batch 157/2005 (epoch 1)\n",
      "[11/18/2025 14:27:21] {train.py:73} INFO - Batch 158/2005 (epoch 1)\n",
      "[11/18/2025 14:27:22] {train.py:73} INFO - Batch 159/2005 (epoch 1)\n",
      "[11/18/2025 14:27:23] {train.py:73} INFO - Batch 160/2005 (epoch 1)\n",
      "[11/18/2025 14:27:24] {train.py:73} INFO - Batch 161/2005 (epoch 1)\n",
      "[11/18/2025 14:27:26] {train.py:73} INFO - Batch 162/2005 (epoch 1)\n",
      "[11/18/2025 14:27:27] {train.py:73} INFO - Batch 163/2005 (epoch 1)\n",
      "[11/18/2025 14:27:28] {train.py:73} INFO - Batch 164/2005 (epoch 1)\n",
      "[11/18/2025 14:27:30] {train.py:73} INFO - Batch 165/2005 (epoch 1)\n",
      "[11/18/2025 14:27:31] {train.py:73} INFO - Batch 166/2005 (epoch 1)\n",
      "[11/18/2025 14:27:32] {train.py:73} INFO - Batch 167/2005 (epoch 1)\n",
      "[11/18/2025 14:27:33] {train.py:73} INFO - Batch 168/2005 (epoch 1)\n",
      "[11/18/2025 14:27:35] {train.py:73} INFO - Batch 169/2005 (epoch 1)\n",
      "[11/18/2025 14:27:36] {train.py:73} INFO - Batch 170/2005 (epoch 1)\n",
      "[11/18/2025 14:27:37] {train.py:73} INFO - Batch 171/2005 (epoch 1)\n",
      "[11/18/2025 14:27:39] {train.py:73} INFO - Batch 172/2005 (epoch 1)\n",
      "[11/18/2025 14:27:40] {train.py:73} INFO - Batch 173/2005 (epoch 1)\n",
      "[11/18/2025 14:27:41] {train.py:73} INFO - Batch 174/2005 (epoch 1)\n",
      "[11/18/2025 14:27:43] {train.py:73} INFO - Batch 175/2005 (epoch 1)\n",
      "[11/18/2025 14:27:44] {train.py:73} INFO - Batch 176/2005 (epoch 1)\n",
      "[11/18/2025 14:27:45] {train.py:73} INFO - Batch 177/2005 (epoch 1)\n",
      "[11/18/2025 14:27:46] {train.py:73} INFO - Batch 178/2005 (epoch 1)\n",
      "[11/18/2025 14:27:48] {train.py:73} INFO - Batch 179/2005 (epoch 1)\n",
      "[11/18/2025 14:27:49] {train.py:73} INFO - Batch 180/2005 (epoch 1)\n",
      "[11/18/2025 14:27:50] {train.py:73} INFO - Batch 181/2005 (epoch 1)\n",
      "[11/18/2025 14:27:51] {train.py:73} INFO - Batch 182/2005 (epoch 1)\n",
      "[11/18/2025 14:27:53] {train.py:73} INFO - Batch 183/2005 (epoch 1)\n",
      "[11/18/2025 14:27:54] {train.py:73} INFO - Batch 184/2005 (epoch 1)\n",
      "[11/18/2025 14:27:55] {train.py:73} INFO - Batch 185/2005 (epoch 1)\n",
      "[11/18/2025 14:27:57] {train.py:73} INFO - Batch 186/2005 (epoch 1)\n",
      "[11/18/2025 14:27:58] {train.py:73} INFO - Batch 187/2005 (epoch 1)\n",
      "[11/18/2025 14:27:59] {train.py:73} INFO - Batch 188/2005 (epoch 1)\n",
      "[11/18/2025 14:28:00] {train.py:73} INFO - Batch 189/2005 (epoch 1)\n",
      "[11/18/2025 14:28:02] {train.py:73} INFO - Batch 190/2005 (epoch 1)\n",
      "[11/18/2025 14:28:03] {train.py:73} INFO - Batch 191/2005 (epoch 1)\n",
      "[11/18/2025 14:28:04] {train.py:73} INFO - Batch 192/2005 (epoch 1)\n",
      "[11/18/2025 14:28:06] {train.py:73} INFO - Batch 193/2005 (epoch 1)\n",
      "[11/18/2025 14:28:07] {train.py:73} INFO - Batch 194/2005 (epoch 1)\n",
      "[11/18/2025 14:28:08] {train.py:73} INFO - Batch 195/2005 (epoch 1)\n",
      "[11/18/2025 14:28:09] {train.py:73} INFO - Batch 196/2005 (epoch 1)\n",
      "[11/18/2025 14:28:11] {train.py:73} INFO - Batch 197/2005 (epoch 1)\n",
      "[11/18/2025 14:28:12] {train.py:73} INFO - Batch 198/2005 (epoch 1)\n",
      "[11/18/2025 14:28:13] {train.py:73} INFO - Batch 199/2005 (epoch 1)\n",
      "[11/18/2025 14:28:15] {train.py:100} INFO - 200 / 100000 | train/accuracy: 53.016 | train/loss: 12.830 | train/stdk: 0.181 | train/stdq: 0.089 | lr: 4.99e-05 | Memory: 12.0 GiB\n",
      "[11/18/2025 14:28:15] {train.py:73} INFO - Batch 200/2005 (epoch 1)\n",
      "[11/18/2025 14:28:16] {train.py:73} INFO - Batch 201/2005 (epoch 1)\n",
      "[11/18/2025 14:28:17] {train.py:73} INFO - Batch 202/2005 (epoch 1)\n",
      "[11/18/2025 14:28:18] {train.py:73} INFO - Batch 203/2005 (epoch 1)\n",
      "[11/18/2025 14:28:20] {train.py:73} INFO - Batch 204/2005 (epoch 1)\n",
      "[11/18/2025 14:28:21] {train.py:73} INFO - Batch 205/2005 (epoch 1)\n",
      "[11/18/2025 14:28:22] {train.py:73} INFO - Batch 206/2005 (epoch 1)\n",
      "[11/18/2025 14:28:24] {train.py:73} INFO - Batch 207/2005 (epoch 1)\n",
      "[11/18/2025 14:28:25] {train.py:73} INFO - Batch 208/2005 (epoch 1)\n",
      "[11/18/2025 14:28:26] {train.py:73} INFO - Batch 209/2005 (epoch 1)\n",
      "[11/18/2025 14:28:28] {train.py:73} INFO - Batch 210/2005 (epoch 1)\n",
      "[11/18/2025 14:28:29] {train.py:73} INFO - Batch 211/2005 (epoch 1)\n",
      "[11/18/2025 14:28:30] {train.py:73} INFO - Batch 212/2005 (epoch 1)\n",
      "[11/18/2025 14:28:31] {train.py:73} INFO - Batch 213/2005 (epoch 1)\n",
      "[11/18/2025 14:28:33] {train.py:73} INFO - Batch 214/2005 (epoch 1)\n",
      "[11/18/2025 14:28:34] {train.py:73} INFO - Batch 215/2005 (epoch 1)\n",
      "[11/18/2025 14:28:35] {train.py:73} INFO - Batch 216/2005 (epoch 1)\n",
      "[11/18/2025 14:28:37] {train.py:73} INFO - Batch 217/2005 (epoch 1)\n",
      "[11/18/2025 14:28:38] {train.py:73} INFO - Batch 218/2005 (epoch 1)\n",
      "[11/18/2025 14:28:39] {train.py:73} INFO - Batch 219/2005 (epoch 1)\n",
      "[11/18/2025 14:28:41] {train.py:73} INFO - Batch 220/2005 (epoch 1)\n",
      "[11/18/2025 14:28:42] {train.py:73} INFO - Batch 221/2005 (epoch 1)\n",
      "[11/18/2025 14:28:43] {train.py:73} INFO - Batch 222/2005 (epoch 1)\n",
      "[11/18/2025 14:28:44] {train.py:73} INFO - Batch 223/2005 (epoch 1)\n",
      "[11/18/2025 14:28:46] {train.py:73} INFO - Batch 224/2005 (epoch 1)\n",
      "[11/18/2025 14:28:47] {train.py:73} INFO - Batch 225/2005 (epoch 1)\n",
      "[11/18/2025 14:28:48] {train.py:73} INFO - Batch 226/2005 (epoch 1)\n",
      "[11/18/2025 14:28:50] {train.py:73} INFO - Batch 227/2005 (epoch 1)\n",
      "[11/18/2025 14:28:51] {train.py:73} INFO - Batch 228/2005 (epoch 1)\n",
      "[11/18/2025 14:28:52] {train.py:73} INFO - Batch 229/2005 (epoch 1)\n",
      "[11/18/2025 14:28:54] {train.py:73} INFO - Batch 230/2005 (epoch 1)\n",
      "[11/18/2025 14:28:55] {train.py:73} INFO - Batch 231/2005 (epoch 1)\n",
      "[11/18/2025 14:28:56] {train.py:73} INFO - Batch 232/2005 (epoch 1)\n",
      "[11/18/2025 14:28:57] {train.py:73} INFO - Batch 233/2005 (epoch 1)\n",
      "[11/18/2025 14:28:59] {train.py:73} INFO - Batch 234/2005 (epoch 1)\n",
      "[11/18/2025 14:29:00] {train.py:73} INFO - Batch 235/2005 (epoch 1)\n",
      "[11/18/2025 14:29:01] {train.py:73} INFO - Batch 236/2005 (epoch 1)\n",
      "[11/18/2025 14:29:03] {train.py:73} INFO - Batch 237/2005 (epoch 1)\n",
      "[11/18/2025 14:29:04] {train.py:73} INFO - Batch 238/2005 (epoch 1)\n",
      "[11/18/2025 14:29:05] {train.py:73} INFO - Batch 239/2005 (epoch 1)\n",
      "[11/18/2025 14:29:06] {train.py:73} INFO - Batch 240/2005 (epoch 1)\n",
      "[11/18/2025 14:29:08] {train.py:73} INFO - Batch 241/2005 (epoch 1)\n",
      "[11/18/2025 14:29:09] {train.py:73} INFO - Batch 242/2005 (epoch 1)\n",
      "[11/18/2025 14:29:10] {train.py:73} INFO - Batch 243/2005 (epoch 1)\n",
      "[11/18/2025 14:29:12] {train.py:73} INFO - Batch 244/2005 (epoch 1)\n",
      "[11/18/2025 14:29:13] {train.py:73} INFO - Batch 245/2005 (epoch 1)\n",
      "[11/18/2025 14:29:14] {train.py:73} INFO - Batch 246/2005 (epoch 1)\n",
      "[11/18/2025 14:29:15] {train.py:73} INFO - Batch 247/2005 (epoch 1)\n",
      "[11/18/2025 14:29:17] {train.py:73} INFO - Batch 248/2005 (epoch 1)\n",
      "[11/18/2025 14:29:18] {train.py:73} INFO - Batch 249/2005 (epoch 1)\n",
      "[11/18/2025 14:29:19] {train.py:73} INFO - Batch 250/2005 (epoch 1)\n",
      "[11/18/2025 14:29:21] {train.py:73} INFO - Batch 251/2005 (epoch 1)\n",
      "[11/18/2025 14:29:22] {train.py:73} INFO - Batch 252/2005 (epoch 1)\n",
      "[11/18/2025 14:29:23] {train.py:73} INFO - Batch 253/2005 (epoch 1)\n",
      "[11/18/2025 14:29:24] {train.py:73} INFO - Batch 254/2005 (epoch 1)\n",
      "[11/18/2025 14:29:26] {train.py:73} INFO - Batch 255/2005 (epoch 1)\n",
      "[11/18/2025 14:29:27] {train.py:73} INFO - Batch 256/2005 (epoch 1)\n",
      "[11/18/2025 14:29:28] {train.py:73} INFO - Batch 257/2005 (epoch 1)\n",
      "[11/18/2025 14:29:30] {train.py:73} INFO - Batch 258/2005 (epoch 1)\n",
      "[11/18/2025 14:29:31] {train.py:73} INFO - Batch 259/2005 (epoch 1)\n",
      "[11/18/2025 14:29:32] {train.py:73} INFO - Batch 260/2005 (epoch 1)\n",
      "[11/18/2025 14:29:33] {train.py:73} INFO - Batch 261/2005 (epoch 1)\n",
      "[11/18/2025 14:29:35] {train.py:73} INFO - Batch 262/2005 (epoch 1)\n",
      "[11/18/2025 14:29:36] {train.py:73} INFO - Batch 263/2005 (epoch 1)\n",
      "[11/18/2025 14:29:37] {train.py:73} INFO - Batch 264/2005 (epoch 1)\n",
      "[11/18/2025 14:29:39] {train.py:73} INFO - Batch 265/2005 (epoch 1)\n",
      "[11/18/2025 14:29:40] {train.py:73} INFO - Batch 266/2005 (epoch 1)\n",
      "[11/18/2025 14:29:41] {train.py:73} INFO - Batch 267/2005 (epoch 1)\n",
      "[11/18/2025 14:29:42] {train.py:73} INFO - Batch 268/2005 (epoch 1)\n",
      "[11/18/2025 14:29:44] {train.py:73} INFO - Batch 269/2005 (epoch 1)\n",
      "[11/18/2025 14:29:45] {train.py:73} INFO - Batch 270/2005 (epoch 1)\n",
      "[11/18/2025 14:29:46] {train.py:73} INFO - Batch 271/2005 (epoch 1)\n",
      "[11/18/2025 14:29:48] {train.py:73} INFO - Batch 272/2005 (epoch 1)\n",
      "[11/18/2025 14:29:49] {train.py:73} INFO - Batch 273/2005 (epoch 1)\n",
      "[11/18/2025 14:29:50] {train.py:73} INFO - Batch 274/2005 (epoch 1)\n",
      "[11/18/2025 14:29:52] {train.py:73} INFO - Batch 275/2005 (epoch 1)\n",
      "[11/18/2025 14:29:53] {train.py:73} INFO - Batch 276/2005 (epoch 1)\n",
      "[11/18/2025 14:29:54] {train.py:73} INFO - Batch 277/2005 (epoch 1)\n",
      "[11/18/2025 14:29:55] {train.py:73} INFO - Batch 278/2005 (epoch 1)\n",
      "[11/18/2025 14:29:57] {train.py:73} INFO - Batch 279/2005 (epoch 1)\n",
      "[11/18/2025 14:29:58] {train.py:73} INFO - Batch 280/2005 (epoch 1)\n",
      "[11/18/2025 14:29:59] {train.py:73} INFO - Batch 281/2005 (epoch 1)\n",
      "[11/18/2025 14:30:00] {train.py:73} INFO - Batch 282/2005 (epoch 1)\n",
      "[11/18/2025 14:30:02] {train.py:73} INFO - Batch 283/2005 (epoch 1)\n",
      "[11/18/2025 14:30:03] {train.py:73} INFO - Batch 284/2005 (epoch 1)\n",
      "[11/18/2025 14:30:04] {train.py:73} INFO - Batch 285/2005 (epoch 1)\n",
      "[11/18/2025 14:30:06] {train.py:73} INFO - Batch 286/2005 (epoch 1)\n",
      "[11/18/2025 14:30:07] {train.py:73} INFO - Batch 287/2005 (epoch 1)\n",
      "[11/18/2025 14:30:08] {train.py:73} INFO - Batch 288/2005 (epoch 1)\n",
      "[11/18/2025 14:30:10] {train.py:73} INFO - Batch 289/2005 (epoch 1)\n",
      "[11/18/2025 14:30:11] {train.py:73} INFO - Batch 290/2005 (epoch 1)\n",
      "[11/18/2025 14:30:12] {train.py:73} INFO - Batch 291/2005 (epoch 1)\n",
      "[11/18/2025 14:30:13] {train.py:73} INFO - Batch 292/2005 (epoch 1)\n",
      "[11/18/2025 14:30:15] {train.py:73} INFO - Batch 293/2005 (epoch 1)\n",
      "[11/18/2025 14:30:16] {train.py:73} INFO - Batch 294/2005 (epoch 1)\n",
      "[11/18/2025 14:30:17] {train.py:73} INFO - Batch 295/2005 (epoch 1)\n",
      "[11/18/2025 14:30:19] {train.py:73} INFO - Batch 296/2005 (epoch 1)\n",
      "[11/18/2025 14:30:20] {train.py:73} INFO - Batch 297/2005 (epoch 1)\n",
      "[11/18/2025 14:30:21] {train.py:73} INFO - Batch 298/2005 (epoch 1)\n",
      "[11/18/2025 14:30:22] {train.py:73} INFO - Batch 299/2005 (epoch 1)\n",
      "[11/18/2025 14:30:24] {train.py:100} INFO - 300 / 100000 | train/accuracy: 53.875 | train/loss: 9.733 | train/stdk: 0.172 | train/stdq: 0.081 | lr: 4.99e-05 | Memory: 12.0 GiB\n",
      "[11/18/2025 14:30:24] {train.py:73} INFO - Batch 300/2005 (epoch 1)\n",
      "[11/18/2025 14:30:25] {train.py:73} INFO - Batch 301/2005 (epoch 1)\n",
      "[11/18/2025 14:30:26] {train.py:73} INFO - Batch 302/2005 (epoch 1)\n",
      "[11/18/2025 14:30:28] {train.py:73} INFO - Batch 303/2005 (epoch 1)\n",
      "[11/18/2025 14:30:29] {train.py:73} INFO - Batch 304/2005 (epoch 1)\n",
      "[11/18/2025 14:30:30] {train.py:73} INFO - Batch 305/2005 (epoch 1)\n",
      "[11/18/2025 14:30:31] {train.py:73} INFO - Batch 306/2005 (epoch 1)\n",
      "[11/18/2025 14:30:33] {train.py:73} INFO - Batch 307/2005 (epoch 1)\n",
      "[11/18/2025 14:30:34] {train.py:73} INFO - Batch 308/2005 (epoch 1)\n",
      "[11/18/2025 14:30:35] {train.py:73} INFO - Batch 309/2005 (epoch 1)\n",
      "[11/18/2025 14:30:37] {train.py:73} INFO - Batch 310/2005 (epoch 1)\n",
      "[11/18/2025 14:30:38] {train.py:73} INFO - Batch 311/2005 (epoch 1)\n",
      "[11/18/2025 14:30:39] {train.py:73} INFO - Batch 312/2005 (epoch 1)\n",
      "[11/18/2025 14:30:40] {train.py:73} INFO - Batch 313/2005 (epoch 1)\n",
      "[11/18/2025 14:30:42] {train.py:73} INFO - Batch 314/2005 (epoch 1)\n",
      "[11/18/2025 14:30:43] {train.py:73} INFO - Batch 315/2005 (epoch 1)\n",
      "[11/18/2025 14:30:44] {train.py:73} INFO - Batch 316/2005 (epoch 1)\n",
      "[11/18/2025 14:30:46] {train.py:73} INFO - Batch 317/2005 (epoch 1)\n",
      "[11/18/2025 14:30:47] {train.py:73} INFO - Batch 318/2005 (epoch 1)\n",
      "[11/18/2025 14:30:48] {train.py:73} INFO - Batch 319/2005 (epoch 1)\n",
      "[11/18/2025 14:30:49] {train.py:73} INFO - Batch 320/2005 (epoch 1)\n",
      "[11/18/2025 14:30:51] {train.py:73} INFO - Batch 321/2005 (epoch 1)\n",
      "[11/18/2025 14:30:52] {train.py:73} INFO - Batch 322/2005 (epoch 1)\n",
      "[11/18/2025 14:30:53] {train.py:73} INFO - Batch 323/2005 (epoch 1)\n",
      "[11/18/2025 14:30:55] {train.py:73} INFO - Batch 324/2005 (epoch 1)\n",
      "[11/18/2025 14:30:56] {train.py:73} INFO - Batch 325/2005 (epoch 1)\n",
      "[11/18/2025 14:30:57] {train.py:73} INFO - Batch 326/2005 (epoch 1)\n",
      "[11/18/2025 14:30:58] {train.py:73} INFO - Batch 327/2005 (epoch 1)\n",
      "[11/18/2025 14:31:00] {train.py:73} INFO - Batch 328/2005 (epoch 1)\n",
      "[11/18/2025 14:31:01] {train.py:73} INFO - Batch 329/2005 (epoch 1)\n",
      "[11/18/2025 14:31:02] {train.py:73} INFO - Batch 330/2005 (epoch 1)\n"
     ]
    }
   ],
   "source": [
    "# training the data\n",
    "# prof mentioned that you should be able to hit 70% accuracy just by training on more data (TA trained for >= 1 day)\n",
    "!bash /users/vishraj/cs511-fall2025-p3-contriever/scripts/train/contriever.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df23f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to delete a model checkpoint\n",
    "# !rm -rf ./model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6732048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model using NQ benchmark\n",
    "#Step1: prepare passage subset\n",
    "#run under contriever/scripts/evaluation/FiD/open_domain_data\n",
    "#place psgs_w100.tsv under contriever/scripts/evaluation/FiD/open_domain_data/\n",
    "# !cp drive/MyDrive/datasets/psgs_w100.tsv drive/MyDrive/contriever/scripts/evaluation/FiD/open_domain_data/\n",
    "\n",
    "!python3 ./scripts/evaluation/FiD/open_domain_data/select_question_subset.py \\\n",
    "--data_dir ./scripts/evaluation/FiD/open_domain_data/download > ./scripts/evaluation/FiD/open_domain_data/passage.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbadcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required passage IDs from ./scripts/evaluation/FiD/open_domain_data/passage.idx...\n",
      "Loaded 92042 required passage IDs\n",
      "\n",
      "Target subset size: 210153 passages (1% of 21015324)\n",
      "Scanning passages file (byte-preserving)...\n",
      "Reading lines: 100%|█████████| 21015324/21015324 [01:12<00:00, 289145.90lines/s]\n",
      "\n",
      "Required passages found: 92042\n",
      "Additional random passages needed: 118111\n",
      "\n",
      "============================================================\n",
      "SUBSET SUMMARY\n",
      "============================================================\n",
      "Total passages in original: 21,015,324\n",
      "Required IDs: 92,042\n",
      "Required passages found: 92,042\n",
      "Final subset size: 210,153\n",
      "Target (1%): 210,153\n",
      "Percentage of original: 1.00%\n",
      "============================================================\n",
      "\n",
      "Saving subset to ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv (byte-identical lines)...\n",
      "✓ Saved 210153 lines to ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv\n"
     ]
    }
   ],
   "source": [
    "!python3 ./scripts/evaluation/FiD/open_domain_data/create_passage_subset.py \\\n",
    "--passage_idx ./scripts/evaluation/FiD/open_domain_data/passage.idx \\\n",
    "--passages_file ./datasets/psgs_w100.tsv \\\n",
    "--output_file ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv \\\n",
    "--seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553977d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading passages from ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv\n",
      "Loading passages: 100%|██████████████| 210154/210154 [00:04<00:00, 44362.46it/s]\n",
      "Loaded 210153 passages\n",
      "Selecting test examples\n"
     ]
    }
   ],
   "source": [
    "#Evaluate your model using NQ benchmark\n",
    "#Step2: preprocess for testing\n",
    "!python3 ./scripts/evaluation/FiD/src/preprocess.py \\\n",
    "--num_questions 1000 --passages_file ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982820af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from ./model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1/checkpoint/lastlog/.\n",
      "Embedding generation for 210153 passages from idx 0 to 210153.\n",
      "Encoding passages: 100%|██████████| 210153/210153 [04:55<00:00, 712.27passage/s]\n",
      "Saving 210153 passage embeddings to ./scripts/evaluation/contriever_embeddings/passages_00.\n",
      "Total passages processed 210153. Written to ./scripts/evaluation/contriever_embeddings/passages_00.\n"
     ]
    }
   ],
   "source": [
    "# Step3: use your model to embed passages\n",
    "!python3 ./scripts/evaluation/generate_passage_embeddings.py  \\\n",
    "--passages ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv \\\n",
    " --model_name_or_path ./model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1/checkpoint/lastlog/  \\\n",
    " --output_dir ./scripts/evaluation/contriever_embeddings  \\\n",
    " --shard_id 0 \\\n",
    " --num_shards 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6d7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model from: ./model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1/checkpoint/lastlog/\n",
      "Indexing passages from files ['./scripts/evaluation/contriever_embeddings/passages_00']\n",
      "Loading file ./scripts/evaluation/contriever_embeddings/passages_00\n",
      "Total data indexed 210153\n",
      "Data indexing completed.\n",
      "Indexing time: 2.1 s.\n",
      "loading passages\n",
      "['./open_domain_data/NQ/test.json']\n",
      "processing path ./open_domain_data/NQ/test.json\n",
      "Questions embeddings shape: torch.Size([1000, 768])\n",
      "Searching for 100 documents in the index\n",
      "100%|███████████████████████████████████████| 1000/1000 [01:22<00:00, 12.15it/s]\n",
      "Search time: 82.3 s.\n",
      "Validation results: top k documents hits %s [50, 82, 114, 130, 148, 169, 183, 202, 219, 231, 241, 252, 261, 267, 275, 289, 295, 308, 312, 322, 326, 331, 335, 341, 343, 349, 352, 356, 363, 368, 371, 373, 381, 388, 393, 396, 402, 403, 407, 410, 414, 414, 419, 422, 424, 427, 430, 431, 436, 441, 444, 444, 448, 448, 449, 450, 452, 454, 457, 458, 463, 465, 468, 469, 469, 470, 472, 472, 473, 473, 475, 477, 477, 479, 480, 485, 486, 487, 488, 491, 492, 493, 494, 498, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 511, 514, 514, 514, 514]\n",
      "R@5: 0.148 R@10: 0.231 R@20: 0.322 R@100: 0.514 \n",
      "Saved results to ./scripts/evaluation/FiD/open_domain_data/contriever_nq/test.json\n"
     ]
    }
   ],
   "source": [
    "# Step4: retrieve the passages\n",
    "#run under scripts/evaluation/FiD/open_domain_data\n",
    "!python3 ./scripts/evaluation/passage_retrieval.py \\\n",
    "--model_name_or_path ./model/average-rmin0.05-rmax0.5-T0.05-4096-0.9995-bert-base-uncased-delete-0.1/checkpoint/lastlog/  \\\n",
    "--passages ./scripts/evaluation/FiD/open_domain_data/psgs_w100_subset.tsv  \\\n",
    "--passages_embeddings ./scripts/evaluation/contriever_embeddings/*  \\\n",
    "--data ./open_domain_data/NQ/test.json \\\n",
    "--output_dir ./scripts/evaluation/FiD/open_domain_data/contriever_nq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3a19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
